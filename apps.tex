\section{Applications}
\label{sec:apps}

The vast majority of the applications that have been deployed on
more than one accelerator have combined graphics engines and reconfigurable
logic as the two types of accelerator.
Tables~\ref{tbl:apps1} and~\ref{tbl:apps2} summarize
the literature for these applications,
indicating the application itself, the year of publication (including
the citation), and various notes about the specific implementation.
Particularly, for each application it is indicated whether (or not)
the application assigns distinct functions to the two different
accelerator types, and also whether (or not) the map-reduce programming
model is used as part of the implementation.

%FPGA and GPU:
%nano sim~\cite{khdo06},
%MC sim~\cite{shsc08,tttl10},
%n-body sim~\cite{tl10},
%crypto~\cite{dfg+13},
%vision~\cite{bkdb10,ghgb11,brf14},
%astronomy (and discussion of dwarfs)~\cite{ibs12},
%astronomy (embedded in instrument)~\cite{kgb+14},
%cardiac optical mapping~\cite{mjk12},
%medical imaging~\cite{szb+12,sll13},
%image processing~\cite{dbd+13}.
%toy MC sim~\cite{admb14},
%BLASTp~\cite{Papad14},
%combo HPC and embedded~\cite{rpm+15},
%image processing in embedded application~\cite{enr+18,nsg+16,zxl+18},
%data acquisition in science~\cite{cab+17,vac+16},
%GWAS~\cite{kws+16},
%GWIS~\cite{wkhe17},
%driver assistance~\cite{wlhk17}
%gene-gene interactions~\cite{wkhe18},

\begin{table}[htp]
\centering
\caption{Applications deployed on multiple accelerators.}
\label{tbl:apps1}
\vspace{0.1in}
\begin{tabular}{p{0.15\linewidth} | c | c | p{0.6\linewidth}}
Application & Ref. & Year & Notes \\ \hline
EM sim & \cite{kdh+06} & 2006 & First known example$^*$ \\ \hline
\multirow{4}{\linewidth}{Monte Carlo simulation} & \cite{ytt+08} & 2008 & European options pricing$^\dag$  \\ \cline{2-4}
 & \cite{shsc08} & 2008 & Financial value-at-risk computation$^*$  \\ \cline{2-4}
 & \cite{tttl10} & 2010 & Asian options and generalized asset pricing \\ \cline{2-4}
 & \cite{admb14} & 2014 & Computation of $\pi$$^*$ \\ \hline
\multirow{3}{\linewidth}{N-body simulation} & \cite{ytt+08} & 2008 & Dynamic workload distribution$^\dag$ \\ \cline{2-4}
 & \cite{tl10} & 2010 & Introduced Axel cluster$^\dag$ \\ \cline{2-4}
 & \cite{sm16} & 2016 & AWS GPUs plus local FPGA \\ \hline
\multirow{2}{\linewidth}{crypto} & \cite{ytt+08} & 2008 & Brute force key search$^\dag$ \\ \cline{2-4}
 & \cite{dfg+13} & 2013 & Password search for PDF documents$^*$ \\ \hline
\multirow{2}{\linewidth}{machine learning} & \cite{bkdb10} & 2010 & Pedestrian detection$^*$ \\ \cline{2-4}
 & \cite{log+18} & 2018 & Character recognition$^*$ \\ \hline
\multirow{3}{*}{vision} & \cite{ghgb11} & 2011 & Stereo 3D video production$^*$ \\ \cline{2-4}
 & \cite{brf14} & 2014 & Hand tracking$^*$ \\ \cline{2-4}
 & \cite{rpm+15} & 2015 & Video coding and motion tracking$^*$ \\ \hline
\multirow{6}{\linewidth}{medical imaging} & \cite{szb+12} & 2012 & Includes both static and dynamic assignment$^*$ \\ \cline{2-4}
 & \cite{mjk12} & 2012 & Cardiac mapping \\ \cline{2-4}
 & \cite{sll13} & 2013 & Transmural electrophysiological imaging$^*$ \\ \cline{2-4}
 & \cite{rpm+15} & 2015 & Cone beam computed tomography$^*$ \\ \cline{2-4}
 & \cite{tzwz15} & 2015 & Cryo-electron microscopy \\ \cline{2-4}
 & \cite{cwl18} & 2018 & Ultrasonic imaging$^*$ \\ \hline
\multirow{6}{\linewidth}{signal and image processing} & \cite{dbd+13} & 2013 & Pedestrian recognition$^*$ \\ \cline{2-4}
 & \cite{rpm+15} & 2015 & Face recognition and character recognition$^*$ \\ \cline{2-4}
 & \cite{akfh15} & 2015 & Real-time RF localization \\ \cline{2-4}
 & \cite{nsg+16} & 2016 & Data acquisition for experimental physics$^*$ \\ \cline{2-4}
 & \cite{enr+18} & 2018 & Add dynamic function distribution to above$^*$\\ \cline{2-4}
 & \cite{zxl+18} & 2018 & Increase frame rate to 9000 frames/s$^*$ \\ \hline
astronomy & \cite{kgb+14} & 2014 & Radio antenna cross-correlation$^*$ \\ \hline
\multirow{4}{\linewidth}{comp bio} & \cite{Papad14} & 2014 & Protein-protein sequence alignment$^*$ \\ \cline{2-4}
 & \cite{kws+16} & 2016 & Genome-wide association studies$^*$ \\ \cline{2-4}
 & \cite{wkhe17} & 2017 & Genome-wide interaction studies$^*$ \\ \cline{2-4}
 & \cite{wkhe19} & 2019 & Computation of gene-gene interactions$^*$ \\ \hline
\end{tabular}
$^*$Assigns distinct functions to the two accelerators. $^\dag$Utilizes map-reduce.
\end{table}

\begin{table}[tp]
\centering
\caption{Applications deployed on multiple accelerators (cont.).}
\label{tbl:apps2}
\vspace{0.1in}
\begin{tabular}{p{0.15\linewidth} | c | c | p{0.6\linewidth}}
Application & Ref. & Year & Notes \\ \hline
\multirow{4}{\linewidth}{data acquisition} & \cite{vac+16} & 2016 & Linear accelerator$^*$ \\ \cline{2-4}
 & \cite{cab+17} & 2017 & Trigger systems for Large Hadron Collider$^*$ \\ \cline{2-4}
 & \cite{hzlw18} & 2018 & 3D waveform oscilloscope \\ \cline{2-4}
 & \cite{crk+19} & 2019 & Linear accelerator$^*$ \\ \hline
\multirow{2}{\linewidth}{driving} & \cite{wlhk17} & 2017 & Driver assistance (lane detection) \\ \cline{2-4}
 & \cite{lzh+18} & 2018 & Autonomous vehicle computations$^*$ \\ \hline
\end{tabular}
$^*$Assigns distinct functions to the two accelerators. $^\dag$Utilizes map-reduce.
\end{table}

A large fraction of the applications listed partition the
workload across distinct functions. Out of 41 total applications, 29
(or about 70\%)
divide the application in this way.  These decisions on the part of the
application developers reflect the reality that the different accelerators
have distinct strengths and weaknesses, and different portions of the
application can differently take advantage of those strengths and/or
ameliorate the weaknesses.

An example of partitioning the workload based on the distinct functions to be
performed is described by Danczul et al.~\cite{dfg+13}. The authors are
attempting a brute force attack on the password for a set of PDF encrypted
files.  This requires both MD5 hash computations as well as RC4 stream cypher 
computations.  They assign the MD5 hashes to the graphics engines, in part
due to its limited memory requirements, and assign the RC4 computations to the
reconfigurable logic (in which the memory requirements of the RC4 algorithm
can be accommodated efficiently).

In a large fraction of the implementations where the workload is
divided by function onto each accelerator, the computation is 
organized as a pipeline.  Data flows into one accelerator, the portion
of the work assigned to that accelerator is performed, the output
of that pipeline stage then flows into the other accelerator, where
the portion of the work assigned to that accelerator is performed.

Sb\^{\i}rlea et al.~\cite{szb+12} extend this to general dataflow graphs,
supporting a richer set of topologies than just a simple pipeline.  The authors
also incorporate the ability to support a work stealing scheduler across
execution platforms.

A clear benefit of the map-reduce paradigm is that it is relatively
straightforward to express the computational parallelism available in
an algorithm using this approach.  Several applications took advantage
of this (4 of 41, or about 10\% of those listed).
In addition, it can also support the dynamic assignment of
workload across the available computational resources, illustrated as
early as 2008 by Yeung et al.~\cite{ytt+08}.
This approach, however, has not been used for recent deployments, likely
due to its inability to effectively exploit the particular advantages
of each accelerator. If the workload is uniform, it stands to reason that
a uniform architecture is the preferred deployment option.

For the set of Monte Carlo simulations listed in the table,
four of the five problems attempted
(\cite{tttl10} includes two) are some form of financial computation.
This hints at the strong interest in accelerated computation on the
part of the financial industry.

Any time multiple compute resources are used in an application, there is
the potential for data movement between those resources to be a performance
bottleneck.  Bittner et al.~\cite{brf14} demonstrate direct graphics engine
to reconfigurable logic DMA data transfers over PCIe, avoiding a copy
to/from the host memory.
Ammendola et al.~\cite{abb+13} also exploit a combination of FPGAs and
graphics engines; however, the FPGAs are not used for application functionality
but instead are used to implement GPU to GPU communications.

In the past decade, there has been a continuation of the research that
seeks to compare one accelerator type with another. Examples of this
that compare reconfigurable logic with graphics engines include:
scientific computations~\cite{wghp11},
compressed sensing and Cholesky decomposition~\cite{ypl12},
sliding-window computations~\cite{cfbs15},
de novo assembly (in computational biology)~\cite{mjk+16},
erasure coding~\cite{czs+16},
database join~\cite{rl17},
and data integration~\cite{fcb+19}.
The quantitative performance comparisons for these experiments are
shown in Table~\ref{tbl:compare2}.
Caraba\~no et al.~\cite{cdde13} compare
energy efficiency, performance, and productivity across accelerators, and
V\'{e}stias and Neto~\cite{vn14} explore accelerator trends, particularly
as they impact both peak performance and sustained performance.
O'Neal and Brisk~\cite{ob18} provide predictive models for each of the
constituent
computational engines: multicores, graphics engines, and reconfigurable
logic, quantifying both performance and power consumption.

\begin{table}[ht]
\centering
\caption{Performance comparisons for graphics engines and reconfigurable
logic, reported since 2010.}
\label{tbl:compare2}
\vspace{0.1in} 
\begin{tabular}{c | c | c | c | c }
\multirow{2}{*}{Application} & \multirow{2}{*}{Ref.} & Graphics & Reconfig. & \multirow{2}{*}{Notes} \\
  &   & Engine & Logic &  \\ \hline
Quantum MC& \multirow{2}{*}{\cite{wghp11}} & \multirow{2}{*}{$50 \times 10^6$} & \multirow{2}{*}{$90 \times 10^6$} & \multirow{2}{*}{interactions/s}\\
simulation  & & & \\ \hline
Cholesky & \multirow{2}{*}{\cite{ypl12}} & \multirow{2}{*}{38$\times$} & \multirow{2}{*}{15$\times$} & \multirow{2}{*}{speedup vs.~1 core}\\
decomposition  & & & \\ \hline
image convolution  & \cite{cfbs15} & 20 & 40 & frames/s\\ \hline
genome alignment & \cite{mjk+16} & 13$\times$ & 115$\times$ & speedup vs.~1 core\\ \hline
erasure coding & \cite{czs+16} & 3.9 & 1.2 & GB/s throughput\\ \hline
database join & \cite{rl17} & 127 & 70 & $\mu$s for 8192 elements\\ \hline
bio data conversion & \cite{fcb+19} & 5.3 & 15.3 & GB/s throughput\\ \hline
ML data conversion & \cite{fcb+19} & 6.2 & 1.8 & GB/s throughput\\
\end{tabular}
\end{table}

A number of investigations have concluded that which accelerator
is preferred often depends upon the properties of the input data.
Examples of applications for which this is true include:
linear algebra~\cite{gchg16,sll+13},
sparse matrix multiplication~\cite{gsbh16},
and vision~\cite{mfo+16}.
In general, the conclusions drawn in the previous decade (see
Section~\ref{sec:compare}) still hold for this work as well.

Graphics engines and reconfigurable logic are not the only accelerators
available, however.  An investigation that included the Cell in
the mix implemented a biological sequence alignment problem~\cite{bal+12}.
Work comparing the Cell with graphics engines (but not including
reconfigurable logic) includes problems such as
wavelet transforms~\cite{bck+11},
Bayesian analysis in bioinformatics~\cite{pts+12},
molecular dynamics~\cite{pts+12}, and the TPC decision support
benchmark~\cite{pts+12}.

Research that compares graphics engines, reconfigurable logic and
the Xeon Phi include an investigation of
sparse matrix multiplication~\cite{gsbh16} as well as the development of
predictive models for power and energy for all three
execution platforms~\cite{opr+17}.
Dropping reconfigurable logic from the comparison yields the following
application investigations:
Ising model simulation~\cite{ws13},
microscopy~\cite{tkk+14},
quantum chemistry~\cite{lrg14},
quantum many-body simulations~\cite{Lyakh15}, and
Jacobi relaxation~\cite{cv16}.
Performance data from some of the above systems is presented in
Table~\ref{tbl:compare3}.
As is the case for the previous comparisons, which accelerator has better
performance is again a function of the properties of the application itself.
Productivity, performance, and energy quantification is pursued
by Memeti et al.~\cite{mlp+17}.

\begin{table}[ht]
\centering
\caption{Performance comparisons for graphics engines and Xeon Phi.}
\label{tbl:compare3}
\vspace{0.1in} 
\begin{tabular}{c | c | c | c | c }
\multirow{2}{*}{Application} & \multirow{2}{*}{Ref.} & Graphics & Xeon & \multirow{2}{*}{Notes} \\
  &   & Engine & Phi &  \\ \hline
sparse matrix& \multirow{2}{*}{\cite{gsbh16}} & \multirow{2}{*}{60} & \multirow{2}{*}{59} & \multirow{2}{*}{Gflops/s}\\
multiply  & & & \\ \hline
Ising model & \multirow{2}{*}{\cite{ws13}} & \multirow{2}{*}{3.72} & \multirow{2}{*}{7.03} & \multirow{2}{*}{ns/spin for $L^{128}$ lattice}\\
simulation  & & & \\ \hline
Jacobi relaxation  & \cite{cv16} & 4.96 & 3.02 & speedup vs.~1 core\\
\end{tabular}
\end{table}

We next turn our focus away from individual applications, and focus
instead on the development environments that enable these applications
to be designed, implemented, deployed, tested, and executed.
