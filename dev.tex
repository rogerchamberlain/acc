\section{Development Tools}
\label{sec:dev}

The general responsibilities associated with application development
tools can be quite broad.  First, they might assist in the expression of
the computation to be executed (either by supporting one or more languages
or introducing a potentially new language).  Second, they might assist the
execution of the application, often through resource management and/or
scheduling.  We will consider each of these in turn.

\subsection{Expression of the Application's Computation}

Here we are interested in tools that help the application
developer express the computation that is to be performed. Essentially, what
must the programmer say to enable the execution platform to do what is
desired.  Traditionally, application expression using even a single
accelerator has been somewhat difficult (e.g., requiring the use of
low-level languages like Verilog and/or VHDL for reconfigurable logic
design), and the addition of a second accelerator with potentially vastly
different properties does nothing to make the task any easier.

Probably the easiest place to make progress on this front is the
development of libraries.  By encapsulating either a portion of the
computation or some support function in a library, the application
developer is relieved of the responsibility to implement that functionality.

As a first example, Thoma et al.~\cite{tdmp15} describe a framework for
supporting accelerator to accelerator communications (specifically supporting
data transfers between GPUs and FPGAs).  In particular, PCIe transfers
are made directly, device-to-device, without data being copied into
the main memory of the multicore host. 

For a second example, Moore et al.~\cite{mlk12} describe an implementation
of the VSIPL++ signal processing library that supports both graphics engines
and reconfigurable logic. The same source code can be used on a multicore
as well as exploit available accelerators.

As a third example, Zhu et al.~\cite{zlwx16} present CNNLab, a framework
for neural network implementations that supports both graphics engines
and reconfigurable logic. Each layer of the model is prescribed via an API.

For reconfigurable logic on the Zynq SoC, Xilinx supports the use of
Python on the embedded processor core for coordination and library
invocation (with the instantiation of the library functionality on the
reconfigurable logic fabric).

Finally, Abalenkovs et al.~\cite{aad+15} explore the performance of
libraries for dense linear algebra deployed on multicores, graphics engines,
and the Intel Xeon Phi.

As a next step in the direction of simplifying application development,
several groups have used what are, in effect, coordination languages.
Here, the individual portions of the overall computation that is to be
performed on an accelerator is programmed using native tools for that
accelerator.  A coordination language is then used to stitch the whole
application together.

Examples of this approach include the Auto-Pipe system, which originally
supported just multicores and reconfigurable logic~\cite{ftb+06}, but was
later expanded to include graphics engines~\cite{cft+10};
the Axel cluster~\cite{tl10}, which initially used the map-reduce
framework for coordination,
extended later to richer coordination capabilities~\cite{ttpl10};
and a data-flow coordination model called Concurrent Collections~\cite{szb+12}.

All of the approaches discussed so far still require some amount of
the application to be developed in the low-level languages supported
by the accelerators (a particularly problematic issue for reconfigurable
logic).
With the availability of OpenCL as a potential common language,
Ahmed~\cite{Ahmed11} built the necessary infrastructure to enable
a single OpenCL application to exploit both graphics engines and
reconfigurable logic for a common application.
The Liquid Metal project~\cite{abb+12} at IBM introduced a new language,
Lime, a Java-compatible object-oriented language, that can be compiled
for and executed on both graphics engines and reconfigurable logic.
In these systems, the reconfigurable logic is expressed in a high-level
language (either OpenCL or Lime, respectively) and a high-level
synthesis process is used to convert the source code into the gate-level
designs necessary for deployment on reconfigurable logic.

Most research into application development languages and systems for
accelerators has focused on one accelerator.  When the target
is a graphics engine, by far the most common two languages are
CUDA and OpenCL. CUDA, being proprietary, is limited to NVIDIA graphics
engines, while OpenCL, as an open standard, can be used across a much
wider set of manufacturers' platforms.

Both CUDA and OpenCL are primarily used to express data parallelism
when targeting graphics engines, particularly regular computations
on densely packed data structures. The MERCATOR system~\cite{cb17}
attempts to widen the scope of applicability to irregular computations,
such as manipulation of sparse data.
While CUDA has been almost exclusively used for graphics engines,
FCUDA~\cite{pgs+13} is a system that does a source-to-source translation
of CUDA code to C (suitable for high-level synthesis), enabling CUDA
applications to be deployed on reconfigurable logic.

OpenACC is a directive-based approach to application expression,
used initially for graphics engines, that has been adapted to support
FPGAs~\cite{lkv16} via source-to-source translation into OpenCL.

While historically the only way to realize efficient and performant
applications on reconfigurable logic was to use a hardware description
language (such as Verilog or VHDL), there are now a number of
options for high-level synthesis tools that enable application expression
at a level of abstraction much closer to that used in graphics engines
or traditional processor cores.  OpenCL as a source language has already
been mentioned above. Many others are based on C/C++ or a subset thereof.
Table~\ref{tbl:tools} summarizes many of these tools,
and Nane et al.~\cite{nsp+16}
provide a broader survey of FPGA-specific tools.
These high-level synthesis tools are now quite effective in
practice, delivering reasonable performance with design effort
comparable to graphics engine application
development~\cite{zms+16,cc19,kenter19}.

\begin{table}[ht]
\centering
\caption{Tools for application expression on accelerators.}
\label{tbl:tools}
\vspace{0.1in} 
\begin{tabular}{c | c | c | c | c }
\multirow{2}{*}{Tool} & \multirow{2}{*}{Ref.} & \multirow{2}{*}{License} & Input & Deployment \\
  &   &   & Language(s) & Target(s)\\ \hline
Auto-Pipe & \cite{cft+10} & academic & VHDL,CUDA & FPGA,GPU \\ \hline
Liquid Metal  & \cite{abb+12} & commercial & Lime & FPGA,GPU \\ \hline
OpenACC & \cite{lkv16} & academic & C/C++ & FPGA,GPU \\ \hline
NVIDIA CUDA  & \cite{gems} & commercial & CUDA & GPU \\ \hline
MERCATOR & \cite{cb17} & academic & CUDA & GPU \\ \hline
DWARV & \cite{nso+12} & academic & C & FPGA \\ \hline
FCUDA & \cite{pgs+13} & academic & CUDA & FPGA \\ \hline
Intel HLS & & commercial & C/C++/OpenCL & FPGA \\ \hline
LegUp & \cite{cca+13} & academic & C & FPGA \\ \hline
Maxeler & \cite{maxj} & commercial & MaxJ & FPGA \\ \hline
ROCCC & \cite{vpnh10} & academic & C & FPGA \\ \hline
Vivado HLS & & commercial & C/C++/OpenCL & FPGA \\
\end{tabular}
\end{table}


While high-level synthesis can be extremely beneficial in terms of
enabling application developers to author their codes at a higher level
of abstraction than was previously available, they are not without 
continuing issues.  Cabrera and Chamberlain~\cite{cc19} recently
investigated the implications of adjusting design parameters in OpenCL 
implementations of the Needleman-Wunsch biosequence alignment algorithm
targeting reconfigurable logic.
They reported performance variability on an Intel FPGA of over 100$\times$
across different parameter configurations.
Knowing how to tune the implementation is clearly crucial to achieving
good performance.

Productivity, performance, and energy quantification is pursued
by Memeti et al.~\cite{mlp+17} for languages that can be deployed
to graphics engines and the Xeon Phi. Their work compared OpenCL,
OpenACC, OpenMP, and CUDA. They developed a developer productivity measure 
that quantifies the fraction of code lines required for parallelization.
Two things they conclude are: (1)~human factors are quite important
(a fact well known in software development circles), and (2)~OpenMP
generally requires less programmer effort that the other approaches.

The European EXTRA project has investigated the issue of design space
exploration for reconfigurable systems~\cite{extra,extra2}. The focus of this
project has been reconfigurability at a courser granularity than traditional
FPGAs (e.g., CGRAs, or course grained reconfigurable arrays), although
the experimental deployments include FPGAs.
A unique feature of this project is the explicit consideration of the
need for effective reconfigurability, i.e., what is deployed on the
accelerator will vary either during the execution of the application or
between applications.
A wide number of applications have been
implemented~\cite{cns+15,zfl+16,akl+17}, as
well as domain specific language approaches to application
expression~\cite{lll16,itl17}.

The take-home message from all of this is that if one's goal is to
develop applications that can be deployed across both graphics engines
and reconfigurable logic, one of the OpenCL variants (from Intel or Xilinx)
is the most appropriate choice.  They both employ knowledge gained from
the various academic efforts, and have matured into well-supported
commercial tools.

\subsection{Managing the Application's Execution}

Given the ability to deploy various components of a problem onto distinct
accelerators, Liu and Luk~\cite{ll11,ll12} provided an early study of how one
should allocate problem components onto execution resources that include
graphics engines and reconfigurable logic.  They explored
three metrics: performance, energy efficiency, and temperature.
This was expanded by Spacey et al.~\cite{slkk13} to describe a formal
model for partitioning tasks across distributed hardware components.
Their experimental work was performed on the Axel cluster~\cite{tl10},
but was limited to multicores and reconfigurable logic, not including
the graphics engines.
L\"{o}sch and Platzner~\cite{lp17} present reMinMin, a scheduling technique
that is focused on optimizing the total energy for a set of tasks executed
across multicores, graphics engines, and reconfigurable logic.  They
report experimental results for four applications, with the heuristic
scheduling algorithm coming within 2\% of optimum in all cases that
were investigated.

Kicherer et al.~\cite{knbk12} come at the problem from a completely
different perspective.  They are interested in the circumstance where an
application is to be executed on any number of different hardware platforms,
some of which include accelerators and some not.  They describe an approach
to enable these applications to be seamlessly portable, taking advantage of
accelerators when present, but still executing correctly when not present.

The above partitioning and mapping approaches result in static assignments.
There has also been some work in the area of dynamic approaches.
Bogda\'{n}ski et al.~\cite{blby11} use on-line machine learning techniques to
dynamically adjust the computation as it proceeds.
Karia and Lopez~\cite{kl17} provide a comprehensive overview of prior
work in scheduling for heterogeneous system, and then propose their own,
Alternative Processor within Threshold, to address some of the issues
present in the prior work.
Both of these groups used simulation
of a multiple accelerator system for their quantitative evaluation.

Belviranli et al.~\cite{bbg13} schedule loop iterations dynamically, resizing
blocks as needed to prevent underutilization and load imbalance.
Bolchini et al.~\cite{bdm+15} describe an approach that dedicates some of the
computational resources to dynamic measurement of the current performance,
feeding that information back into the scheduling decisions, effectively
making the system self-adaptive.
Both of the above groups
evaluate their approaches on a pair of systems, one containing graphics
engines and the other containing reconfigurable logic, but don't include
a machine that contains both accelerators.

An approach that addresses both the issues of application expression
and application execution is EngineCL~\cite{dng+19}.  EngineCL is a
high-level framework that provides load balancing across devices that
are authored in OpenCL.  An experimental comparison of static versus
dynamic load balancing approaches yields strong evidence for the
improvements that are realizable with dynamic scheduling.

%Choose accelerator based on properties of input:
%QUARK~\cite{hcy+14,hjl+15}.
