\section{Early History}
\label{sec:history}

Hardware acceleration of applications has a long history.
In 1984, Blank~\cite{Blank84}
published a survey of hardware accelerators used in computer-aided design 
that described a set of special-purpose engines going back nearly 20 years
(the first being described in 1969~\cite{McKay69}).
Blank describes accelerators for logic simulation, design rule checking,
placement, and routing. These are all tasks that remain computationally
expensive today, especially as digital systems designs have grown in size
and complexity.

These accelerators were all dedicated hardware engines, capable only
of executing the specific application for which they were originally designed
and built. As a result, none of them were economically viable over any
length of time.  During this time period, the march of technology (specifically
the combination of Moore's Law and Dennard Scaling) quickly
overcame any functionality-limited design.  It simply wasn't long
before general-purpose machines became fast enough to eliminate the
performance advantage that was available via hardware specialization.

While the example given above is computer-aided design, there were similar
efforts in other domains (e.g., N-body simulations in astrophysics~\cite{grape},
LISP machines~\cite{lisp,alphalisp},
Java machines~\cite{java,Schoeberl08}),
all of which suffered a similar fate.

\subsection{Comparing Different Accelerators}

There has been a significant amount of work dedicated to effectively
using accelerators on a wide range of problems.  Excellent reviews
include those by Mittal and Vetter~\cite{mv15} for graphics engines,
Escobar et al.~\cite{ecv16} for reconfigurable logic, and
Brodtkorb et al.~\cite{bdh+10} for three accelerators: graphics engines,
reconfigurable logic, and Cell processors.

Approximately a decade ago, there was a fair amount of research that
went into comparing one accelerator with another.  The bulk of that
work compared graphics engines to reconfigurable logic on a wide range
of applications and/or application kernels.
Examples include dense matrix multiplication~\cite{cmhm10,jpbc10},
sparse matrix-vector multiplication~\cite{sww+10},
Gaussian elimination~\cite{cls+08},
FFT~\cite{cmhm10},
reduction~\cite{jpbc10},
encryption~\cite{cls+08},
image processing~\cite{amy09,bnw+10},
lithography simulation~\cite{cz09},
pseudo-random number generation~\cite{jpbc10,tb09,thl09},
N-body simulation~\cite{jpbc10},
computational biology~\cite{cls+08}, and
computational finance~\cite{cmhm10,tb10}.

Kapre and DeHon~\cite{kd09} compared the performance of graphics engines,
reconfigurable logic, and Cell processors on a circuit simulation application,
and Baker et al.~\cite{bgt07} did the same exploring matched filter
computations.

Table~\ref{tbl:compare} summarizes the majority of the
reported results from these studies.
The clear takeaway from this table is that there is no clear winner
for all of these applications which accelerator gives the greater performance.
It clearly depends on the properties of the application and the implementation.

\begin{table}[ht]
\centering
\caption{Performance comparisons for different accelerators on a variety of applications, all reported between 2007 and 2010.}
\label{tbl:compare}
\vspace{0.1in} 
\begin{tabular}{c | c | c | c | c }
\multirow{2}{*}{Application} & \multirow{2}{*}{Ref.} & Graphics & Reconfig. & \multirow{2}{*}{Notes} \\
  &   & Engine & Logic &  \\ \hline
matrix & \cite{cmhm10} & 541 & 1,491 & GFLOP/s\\
multiply  & \cite{jpbc10} & 120$\times$ & 60$\times$ & speedup vs.~1 core\\ \hline
reduction  & \cite{jpbc10} & 120$\times$ & 80$\times$ & speedup vs.~1 core\\ \hline
image & \cite{amy09} & 100 & 600 & frames/s \\
processing & \cite{bnw+10} & 847 & 258 & frames/s \\ \hline
lithography sim. & \cite{cz09} & 8$\times$ & 15$\times$ & speedup vs.~1 core\\ \hline
pseudo-random & \cite{jpbc10} & 90$\times$ & 89$\times$ & speedup vs.~1 core\\ 
number  & \cite{tb09} & $3\times10^9$ & $26\times10^9$ & random numbers/s\\
generation & \cite{thl09} & $14\times10^9$ & $44\times10^9$ & random numbers/s\\ \hline
N-body sim.  & \cite{jpbc10} & 70$\times$ & 5$\times$ & speedup vs.~1 core\\ \hline
comp.& \cite{cmhm10} & 10,756 & 7,800 & Mopts/s\\
finance  & \cite{tb10} & 50$\times$ & 162$\times$ & speedup vs.~1 core\\ \hline
circuit sim. & \cite{kd09} & 131$\times$ & 182$\times$ & speedup vs.~1 core\\ \hline
matched filter  & \cite{bgt07} & 6$\times$ & 2$\times$ & speedup vs.~1 core\\
\end{tabular}
\end{table}

\subsection{First Use of Multiple Accelerators on a Common Problem}

The first known (to this author) combination of a graphics engine and
reconfigurable logic used on a common problem was presented
in two papers
by Kelmelis et al.~\cite{khdo06,kdh+06} in 2006.  They used a pair of PCI cards
(an EM~Photonics Celerity card with a Xilinx Virtex-II FPGA and
an NVIDIA GeForce 7800 GTX graphics card) to accelerate the execution of
the finite-difference time-domain (FDTD) simulation of electromagnetic
waves~\cite{khdo06} and nanoscale devices~\cite{kdh+06}.
In the electromagnetic wave simulation, the 2D problem was solved using
the graphics engine, exploiting its greater memory bandwidth, while the 3D
problem was solved using the reconfigurable logic, building a custom cache.
As such, this problem doesn't yet meet our interest in using both
accelerators on a common problem.

In~\cite{kdh+06}, the authors describe what is believed to be the first
ever use of multicores, graphics engines, and reconfigurable logic to execute
a single application.  In the problem partitioning, the graphics engine is
responsible for the mesh construction over the 3D space, which is a regular
computation that maps well onto the graphics engine's data parallel pipelines.
The reconfigurable logic is responsible for performing the field-update
computations, which benefits from the custom cache designed to provide input
data to the field updates.

This combined machine was then used to execute a full-wave electromagnetic
circuit simulation in which the graphics engine was used for coefficient
calculations and matrix setup while the reconfigurable logic was responsible
for the iterative matrix solver.

\subsection{Other Early Uses of Multiple Accelerators}

The second known example of using multiple accelerators on a common
problem is the deployment of a financial Monte Carlo simulation 
(a value-at-risk computation) on
graphics engines and reconfigurable logic by Singla et al.~\cite{shsc08},
described in 2008.
The authors report a speedup of 74$\times$ relative to an 8-core implementation,
using an NVIDIA GeForce GTX 260 as the graphics processor and
a Xilinx Virtex-4 LX80 FPGA for the reconfigurable logic.
The processors are 2.2~GHz AMD Opterons.

In 2010, there were two publications that describe such deployments.
First, Tsoi and Luk~\cite{tl10} deployed an N-body simulation application
on the Axel cluster, a heterogeneous system in which each cluster node
includes both reconfigurable logic and a graphics engine. The map-reduce
framework is used to deploy applications on the Axel system.

Second, Bauer et al.~\cite{bkdb10} deploy a video-based pedestrian
detection problem on a PC system that includes a graphics card and
in which the frame grabber board has an embedded reconfigurable logic chip.
The reconfigurable logic is responsible for feature extraction and the
graphics engine is responsible for classification (using a Gaussian
kernel support vector machine).

Development environment~\cite{cft+10},
included FPGA hardware but no empirical FPGA results~\cite{dy08}.

Cluster~\cite{tl10}, cluster (w/GRAPE)~\cite{sbm+09}.

Map-reduce~\cite{ytt+08} (both, but not simultaneously).

