\section{Early History}
\label{sec:history}

Hardware acceleration of applications has a long history.
In 1984, Blank~\cite{Blank84}
published a survey of hardware accelerators used in computer-aided design 
that described a set of special-purpose engines going back nearly 20 years
(the first being described in 1969~\cite{McKay69}).
Blank describes accelerators for logic simulation, design rule checking,
placement, and routing. These are all tasks that remain computationally
expensive today, especially as digital systems designs have grown in size
and complexity.

These accelerators were all dedicated hardware engines, capable only
of executing the specific application for which they were originally designed
and built. As a result, none of them were economically viable over any
length of time.  During this time period, the march of technology (specifically
the combination of Moore's Law and Dennard Scaling) quickly
overcame any functionality-limited design.  It simply wasn't long
before general-purpose machines became fast enough to eliminate the
performance advantage that was available via hardware specialization.

While the example given above is computer-aided design, there were similar
efforts in other domains (e.g., N-body simulations in
astrophysics~\cite{grapenature,grape},
LISP machines~\cite{lisp,alphalisp},
Java machines~\cite{java,Schoeberl08}),
all of which suffered a similar fate.

\subsection{Comparing Different Accelerators}

There has been a significant amount of work dedicated to effectively
using accelerators on a wide range of problems.  Excellent reviews
include those by Mittal and Vetter~\cite{mv15} for graphics engines,
Escobar et al.~\cite{ecv16} for reconfigurable logic, and
Brodtkorb et al.~\cite{bdh+10} for three accelerators: graphics engines,
reconfigurable logic, and Cell processors.
Chung et al.~\cite{cmhm10} provided a retrospective of the field in 2010.

Approximately a decade ago, there was a fair amount of research that
went into comparing one accelerator with another.  The bulk of that
work compared graphics engines to reconfigurable logic on a wide range
of applications and/or application kernels.
Examples include dense matrix multiplication~\cite{cmhm10,jpbc10},
sparse matrix-vector multiplication~\cite{sww+10},
Gaussian elimination~\cite{cls+08},
FFT~\cite{cmhm10},
reduction~\cite{jpbc10},
encryption~\cite{cls+08},
image processing~\cite{amy09,bnw+10},
lithography simulation~\cite{cz09},
pseudo-random number generation~\cite{jpbc10,tb09,thl09},
N-body simulation~\cite{jpbc10},
computational biology~\cite{cls+08}, and
computational finance~\cite{cmhm10,tb10}.

Kapre and DeHon~\cite{kd09} compared the performance of graphics engines,
reconfigurable logic, and Cell processors on a circuit simulation application,
and Baker et al.~\cite{bgt07} did the same exploring matched filter
computations.

Table~\ref{tbl:compare} summarizes the majority of the
reported results from these studies.
The clear takeaway from this table is that there is no clear winner
for all of these applications which accelerator gives the greater performance.
It clearly depends on the properties of the application and the implementation.

\begin{table}[ht]
\centering
\caption{Performance comparisons for different accelerators on a variety of applications, all reported between 2007 and 2010.}
\label{tbl:compare}
\vspace{0.1in} 
\begin{tabular}{c | c | c | c | c }
\multirow{2}{*}{Application} & \multirow{2}{*}{Ref.} & Graphics & Reconfig. & \multirow{2}{*}{Notes} \\
  &   & Engine & Logic &  \\ \hline
matrix & \cite{cmhm10} & 541 & 1,491 & GFLOP/s\\
multiply  & \cite{jpbc10} & 120$\times$ & 60$\times$ & speedup vs.~1 core\\ \hline
reduction  & \cite{jpbc10} & 120$\times$ & 80$\times$ & speedup vs.~1 core\\ \hline
image & \cite{amy09} & 100 & 600 & frames/s \\
processing & \cite{bnw+10} & 847 & 258 & frames/s \\ \hline
lithography sim. & \cite{cz09} & 8$\times$ & 15$\times$ & speedup vs.~1 core\\ \hline
pseudo-random & \cite{jpbc10} & 90$\times$ & 89$\times$ & speedup vs.~1 core\\ 
number  & \cite{tb09} & $3\times10^9$ & $26\times10^9$ & random numbers/s\\
generation & \cite{thl09} & $14\times10^9$ & $44\times10^9$ & random numbers/s\\ \hline
N-body sim.  & \cite{jpbc10} & 70$\times$ & 5$\times$ & speedup vs.~1 core\\ \hline
comp.& \cite{cmhm10} & 10,756 & 7,800 & Mopts/s\\
finance  & \cite{tb10} & 50$\times$ & 162$\times$ & speedup vs.~1 core\\ \hline
circuit sim. & \cite{kd09} & 131$\times$ & 182$\times$ & speedup vs.~1 core\\ \hline
matched filter  & \cite{bgt07} & 6$\times$ & 2$\times$ & speedup vs.~1 core\\
\end{tabular}
\end{table}

While which accelerator will win a head-to-head performance competition varies
with application and implementation, a number of clear trends are evident
from the work described in this section.  In particular, the three types
of compute engine (multicores, graphics engines, and reconfigurable logic)
each have their own strengths and weaknesses relative to the other two.
These can be briefly stated as follows:
\begin{itemize}
\item {\bf Multicores} -- Strengths: generality, application portability,
ease of programmability. Weaknesses: lowest parallelism of the group,
rigid memory subsystem, low power efficiency.
\item {\bf Graphics engines} -- Strengths: high parallelism (esp.~for data
parallel problems), high density of floating-point units, high throughput
memory (including coalesing). Weaknesses: rigid computational data path,
limited suitability for a variety of applications.
\item {\bf Reconfigurable logic} -- Strengths: extremely high flexibility,
suitability for fine-grain parallelism, power efficiency.  Weaknesses: 
difficulty of programmability, lower clock rate (necessitating greater
parallelism to achieve similar performance).
\end{itemize}

While the above conclusions were based upon investigations completed
by 2010, they are still as true today as they were then.

\subsection{First Use of Multiple Accelerators on a Common Problem}

The first known (to this author) combination of a graphics engine and
reconfigurable logic used on a common problem was presented
in two papers
by Kelmelis et al.~\cite{khdo06,kdh+06} in 2006.  They used a pair of PCI cards
(an EM~Photonics Celerity card with a Xilinx Virtex-II FPGA and
an NVIDIA GeForce 7800 GTX graphics card) to accelerate the execution of
the finite-difference time-domain (FDTD) simulation of electromagnetic
waves~\cite{khdo06} and nanoscale devices~\cite{kdh+06}.
In the electromagnetic wave simulation, the 2D problem was solved using
the graphics engine, exploiting its greater memory bandwidth, while the 3D
problem was solved using the reconfigurable logic, building a custom cache.
As such, this problem doesn't yet meet our interest in using both
accelerators on a common problem.

In~\cite{kdh+06}, the authors describe what is believed to be the first
ever use of multicores, graphics engines, and reconfigurable logic to execute
a single application.  In the problem partitioning, the graphics engine is
responsible for the mesh construction over the 3D space, which is a regular
computation that maps well onto the graphics engine's data parallel pipelines.
The reconfigurable logic is responsible for performing the field-update
computations, which benefits from the custom cache designed to provide input
data to the field updates.

This combined machine was then used to execute a full-wave electromagnetic
circuit simulation in which the graphics engine was used for coefficient
calculations and matrix setup while the reconfigurable logic was responsible
for the iterative matrix solver.

\subsection{Other Early Uses of Multiple Accelerators}

A second example of simultaneous use of multiple accelerators was
reported by Yeung et al.~\cite{ytt+08} in 2008.
The authors propose the use of the map-reduce framework for heterogeneous
systems, show the performance of a number of benchmarks (e.g., Monte Carlo
simulation, European options pricing, cipher attack, N-body simulation)
using each accelerator independently, and then show performance with
combined use of both the graphics engine and the reconfigurable logic.

The third known example of using multiple accelerators on a common
problem is the deployment of a financial Monte Carlo simulation 
(a value-at-risk computation) on
graphics engines and reconfigurable logic by Singla et al.~\cite{shsc08},
also described in 2008.
The authors report a speedup of 74$\times$ relative to an 8-core implementation,
using an NVIDIA GeForce GTX 260 as the graphics processor and
a Xilinx Virtex-4 LX80 FPGA for the reconfigurable logic.
The processors are 2.2~GHz AMD Opterons.

In 2010, there were three publications that describe such deployments.
First, Tsoi and Luk~\cite{tl10} deployed an N-body simulation application
on the Axel cluster, a heterogeneous system in which each cluster node
(out of 16 total)
includes both reconfigurable logic and a graphics engine. The map-reduce
framework is used to deploy applications on the Axel system.

Second, Bauer et al.~\cite{bkdb10} deployed a video-based pedestrian
detection problem on a PC system that includes a graphics card and
in which the frame grabber board has an embedded reconfigurable logic chip.
The reconfigurable logic is responsible for feature extraction and the
graphics engine is responsible for classification (using a Gaussian
kernel support vector machine).

Third, Tse et al.~\cite{tttl10} deployed a pair of Monte Carlo simulation
applications on a cluster with 8 Virtex-5 FPGAs and 8 Tesla C1060 GPUs.
The asset simulation application achieved a speedup of 44$\times$ relative
to a 16-node cluster of AMD quad-core 2.4~GHz multicores.  Energy efficiency
improved almost 20-fold.

\subsection{Early Machines and Development Environments}

In addition to the Axel cluster~\cite{tl10} mentioned above, another
early machine that included both graphics engines and reconfigurable
logic is the Quadro Plex (QP)~\cite{sep+09} cluster at NCSA.
The QP, deployed in 2007,
was a 16 node cluster in which each compute node contained two
dual-core 2.4~GHz AMD Opterons, four NVIDIA Quadro FX 5600 graphics
engines and one Nallatech H101-PCIX reconfigurable logic accelerator card.
While~\cite{sep+09} describes several applications deployed on the cluster,
they all appear to use one accelerator or the other, but not both.

Kastl and Loimayr~\cite{kl10} described a machine designed to accelerate
cryptanalytic algorithms.  However, in the initial publication the
reconfigurable logic was not operational and any application testing
was limited to graphics engine acceleration.

Early development environments include Auto-Pipe~\cite{ftb+06,cft+10}
and Harmony~\cite{dy08}.  Auto-Pipe was originally introduced in 2006
as an environment for streaming applications supporting both traditional
multicores and reconfigurable logic~\cite{ftb+06}, and was later extended
to support graphics engines~\cite{cft+10}. This was the environment
used by Singla et al.~\cite{shsc08}.

The Harmony environment supports applications that have been decomposed into
\emph{computer kernels} whose execution can be subject to explicit
\emph{control decisions}~\cite{dy08}.
As such, it has much in common with Auto-Pipe's
streaming model, with a slightly richer set of topologies supported.
While the design of Harmony includes reconfigurable logic support, the
experimental evaluation provided in~\cite{dy08} is limited to multicores
and graphics engines.

The work described in this section was all published by 2010. In the
sections that follow, we will concentrate on research that has been
published since then.
