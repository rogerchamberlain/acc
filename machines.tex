\section{Machines}
\label{sec:machines}

The ability to physically construct a machine with multiple accelerators
has never really been in question.  Frankly, they are straightforward to
construct using readily available commercial components.
Figure~\ref{fig:nodearch} illustrates the classic approach to such a design.
Clusters of nodes of this style can be readily constructed using either
Ethernet or Infiniband as the communication technology.

\begin{figure}[ht]
\centering
\includegraphics[width=0.75\linewidth]{nodearch}
\caption{Individual node architecture for an multiple-accelerator system.}
\label{fig:nodearch}
\end{figure}

In the figure, a multicore host is attached to a reconfigurable logic board
and to a graphics engine via the host's I/O bus.  Current systems use
PCIe as the I/O bus, however, most of the early systems described in
the previous section used PCI-X.
Each of the elements of the machine (the multicore, the reconfigurable
logic, and the graphics engine) has its own physical memory.  It is common
for reconfigurable logic boards to have both SRAM and DRAM on board.
Since there is not, by default, a common address space that encompasses
all of these physical memories, data communications between execution
engines can be a significant issue both in application development and
performance.  We will come back to this point later.

After the initial set of machines described in Section~\ref{sec:em} above,
a number of additional machines have been designed, built, and deployed
that support multiple accelerators.

In 2012, the Chimera system was described~\cite{ibs12}.
It combined Intel multicores, NVIDIA Tesla C2070 graphics engines,
and an Altera (now Intel) Stratix-IV reconfigurable logic card using
the motherboard's PCIe bus. The authors describe approaches to deploying
a variety of applications on the system. However, performance results
are not provided.

A multiple accelerator cluster built at the Center for Development of
Advanced Computing (C-DAC) in Bangalore, India, was described in
2014~\cite{admb14}. Here, the authors adapted the StarPU development
environment~\cite{starpu} to support reconfigurable logic as well as
simultaneous support of CUDA and OpenCL in the same application.
A Monte Carlo simulation application is described, achieving a 22$\times$
speedup relative to a Xeon processor.

Also in 2014, Wu et al.~\cite{whk+14} describe a heterogeneous compute
platform with multicores, graphics engines, and reconfigurable logic
that has a focus on power efficient computing.
The authors explore four application examples, but similar to the QP
machine, chose to exploit one accelerator type or another, but not both
simultaneously.

Proa\~no et al.~\cite{pcc14} describe an open-source framework for
integrating accelerators into a cloud infrastructure.  Their goal is
to enable both graphics engines and reconfigurable logic as compute resources
in the Infrastructure as a Service (IaaS) cloud service model.
With AWS having already announced the availability of graphics engines
in the cloud, the authors focused on exploring the issues associated
with enabling the reconfigurable logic.
More recently, AWS has announced the availability of FPGA-enabled nodes
in their cloud infrastructure.

In 2015, Rethinagiri et al.~\cite{rpm+15} described a pair of computational
platforms that included both graphics engines and reconfigurable logic.  One
was targeting high-performance computing (HPC) applications and the other
was targeting high-performance embedded (HPE) applications.
In both cases, their interest was a combination of performance and
energy efficiency. Table~\ref{tbl:hpchpe} gives the particulars of
the two platforms (extracted from Table~I of~\cite{rpm+15}).

\begin{table}[ht]
\centering
\caption{Specifications for HPC and HPE platforms~\protect\cite{rpm+15}.}
\label{tbl:hpchpe}
\vspace{0.1in}
\begin{tabular}{c | c | c | c}
Domain & Engine & Specification & Communication \\ \hline
\multirow{3}{*}{HPC} & cores & Intel Xeon E5-2634 & Gen3 $\times$16 \\ \cline{2-4}
 & GPU & NVIDIA Telsa K40 & Gen3 $\times$16 \\ \cline{2-4}
 & FPGA & Xilinx Virtex-7 VC709 & Gen3 $\times$8 \\ \hline
\multirow{2}{*}{HPE} & cores and GPU & NVIDIA Jetson TK1 & Gen2 $\times$1 \\ \cline{2-4}
 & cores and FPGA & Xilinx Zynq-7015 & Gen2 $\times$4
\end{tabular}
\end{table}

The authors give performance results and energy efficiency for five
applications across the pair of machines:
computed tomography, face recognition, video coding, character recognition,
and motion tracking.

The SuperDragon system~\cite{tzwz15} contains 64 nodes, each of which is
comprised of an Intel Westmere processor, an NVIDIA Tesla C2050 graphics
engine, and a pair of Xilinx FPGAs (an XC5VLX330-1 for computation and an
XC5VLX70T for control and communications management), connected by
PCIe. The 64 nodes communicate via
an Infiniband network. The authors use a Cryo-electron microscopy
application to explore various mapping options for assigning work to the
different accelerators.

Segal and Margala~\cite{sm16} describe a system constructed using commodity cloud
nodes (from AWS) and a locally deployed node containing a reconfigurable logic
card.  They used the SparkCL framework~\cite{sparkcl}, which adds accelerator
support to Apache Spark, to deploy an N-body simulation. They conclude that
which accelerator gives the best performance depends in part on the dynamics
of the particle interactions in the N-body simulation.

This latter point is worth reinforcing. Not only does the application
algorithm impact what type of accelerator gives the best performance, for
many problems the nature of the input data can also influence this result.
In many cases, it might very well be impractical to know, for certain,
which accelerator is to be preferred prior to the actual execution.
This leaves the door open for adaptive approaches to be effective.

Finally, in 2016 Contassot-Vivier and Vialle~\cite{cv16} described
a pair of machines
that contain both an NVIDIA graphics engine and also a Xeon Phi. This is
the first known combination of these two accelerators in a single machine.
The authors give performance data for a Jacobi relaxation application
on each of these two machines.
They report that effective management of communications (between host and
accelerators) is essential to achieving good performance on this type of
machine.
